{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 준비 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(target) :\n",
    "    iris = load_iris() \n",
    "    X_tr = iris.data[:, 2:] # 4개의 특징 중 꽃잎의 길이와 폭 선택\n",
    "    labels = iris.target_names # iris data가 sklearn.datasets.data 형태여서 target_naems라는 것이 정의되어있음\n",
    "    y = iris.target\n",
    "\n",
    "    # 학습 표본의 레이블 지정 - target에 지정된 레이블이면 1, 그 외는 0\n",
    "    y_tr = []\n",
    "    for i in range(150) : # iris data 갯수 150\n",
    "            y_tr.append(labels[y[i]] == target) \n",
    "        y_tr = np.array(y_tr, dtype=int)\n",
    "        return X_tr, y_tr ['(1) '+target, '(0) the others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 활성함수 - 시그모이드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "    ''' x : numpy array '''\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 손실함수 - mse,corss entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(y, y_hat) :\n",
    "    loss = 0.0\n",
    "    for i in range(len(y)):\n",
    "        err = y_hat[i] - y[i]\n",
    "        loss += np.dot(err,err)\n",
    "    return loss / len(y) #평균제곱오차\n",
    "\n",
    "def loss_ce(y, y_hat):\n",
    "    loss = 0.0\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1: #y가 1차원배열 or 두번째차원 크기가 1 Binary cross-entropy case\n",
    "        for i in range(len(y)) :\n",
    "            loss += -(y[i] * np.log(y_hat[i])\n",
    "                      + (1-y[i]) * np.log((1-y_hat[i]))).sum()\n",
    "    else : # Categorical cross-entropy case\n",
    "        for i in range(len(y)):\n",
    "            loss += -(y[i] * np.log(y_hat[i])).sum()\n",
    "    return loss / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] Dense 클래스 - 완전연결층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, nIn, nOut, activation='sigmoid', loss='mse'):\n",
    "        self.nIn = nIn # 입력의 수\n",
    "        self.nOut = nOut # 출력의 수\n",
    "        # 가중치(w)와 바이어스(b)를 He normal 방식으로 초기화\n",
    "        rnd = np.random.default_rng()\n",
    "        self.w == rnd.normal(scale = np.sqrt(2.0/ self.nIn),\n",
    "                             size = (self.nOut, self.nIn)).astype(np.float32)\n",
    "        self.b == rnd.normal(scale = np.sqrt(2.0/ self.nIn),\n",
    "                             size = self.nOut).astype(np.float32)\n",
    "        # 활성함수 설정\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            if loss =='ce':    self.dE_du = self.dE_du_sigmoid_ce\n",
    "            else :              self.dE_du = self.dE_du_sigmoid_mse\n",
    "            self.do_du = self.do_du_sigmoid\n",
    "        # 모멘텀을 적용하기 위한 속도의 초깃값 설정\n",
    "        self.velocity_w, self.velocity_b = 0.0, 0.0\n",
    "    \n",
    "    # 입력 X에 대한 출력 계산\n",
    "    def output(self,X):\n",
    "        self.in_vec = X #bp 학습을 위해 입력 보관\n",
    "        # 입력의 가중 합 계산\n",
    "        u = np.array([np.dot(self.w[i], X) + self.b[i]\n",
    "                      for i in range(self.nOut)], dtype=np.float32)\n",
    "        # 활성함수를 적용한 출력 계산\n",
    "        self.out_vec = self.activation(u) #bp 학습을 위해 출력 보관\n",
    "        return self.out_vec\n",
    "    \n",
    "    # 경사 하강법에 따라 w 및 b 갱신\n",
    "    def gd(self, dw, db, momentum=0):\n",
    "        self.velocity_w = self.velocity_w * momentum -dw\n",
    "        self.velocity_b = self.velocity_b * momentum -db\n",
    "        self.w += self.velocity_w\n",
    "        self.b += self.velocity_b\n",
    "\n",
    "    def dE_du_sigmoid_mse(self, y):\n",
    "        return (self.out_vec - y)\n",
    "    \n",
    "    def do_du_sigmoid(self):\n",
    "        return self.out_vec * (1-self.out_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] BP_Model : BP(Backpropagation) 학습을 하는 피드포워드 모델 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BP_Model():\n",
    "    def __init__(self, nUnitLst, loss='mse',\n",
    "                 activation_h='siogmoid', activation_o='sigmoid'):\n",
    "        layers = []\n",
    "        self.nLayers = len(nUnitLst) -1 \n",
    "        # 은닉층 구성\n",
    "        for i in range(self.nLayers -1):\n",
    "            layers.append(Dense(nUnitLst[i], nUnitLst[i+1],\n",
    "                            activation=activation_h, loss=loss))\n",
    "        # 출력층 구성\n",
    "        layers.append(Dense(nUnitLst[self.nLayers-1],\n",
    "                            nUnitLst[self.nLayers], activawtion=activation_o, loss=loss))\n",
    "        self.layers = np.array(layers, dtype=object)\n",
    "        self.ohe = np.identity(nUnitLst[-1]) # identity 항등행렬\n",
    "        if loss == 'ce':\n",
    "            self.loss = loss_ce\n",
    "        else:\n",
    "            self.loss = loss_mse\n",
    "    \n",
    "    def predict(self, x):\n",
    "        res = []\n",
    "        for j in range(len(x)):\n",
    "            xx = x[j]\n",
    "            for i in range(self.nLayers):\n",
    "                xx = self.layers[i].output(xx)\n",
    "            res.append(xx)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def fit(self, X, y, N, epochs, eta=0.01, momentum=0):\n",
    "        # 학습표본의 인덱스를 무작위 순서로 섞음\n",
    "        idx = list(range(N))\n",
    "        np.random.shuffle(idx)\n",
    "        X = np.array([X[idx[i]] for i in range(N)])\n",
    "        if self.layers[self.nLayers-1].nOut == 1:\n",
    "            y = np.array([y[idx[i]] for i in range(N])\n",
    "        else:\n",
    "            y = np.array([self.ohe[y[idx[i]]] for i in range(N)])\n",
    "        \n",
    "        f = 'Epochs = {:4d}    Loss = {8.5f}'\n",
    "        # w와 b의 변화량을 저장할 수 있게 준비함\n",
    "        dw,db = [], []\n",
    "        for i in range(self.nLayers):\n",
    "            dw.append(np.zeros((self.layers[i].nOut,\n",
    "                               self.layers[i].nIn), dtype=np.float32))\n",
    "            db.append(np.zeros(self.layers[i].nOut, dytype=np.float32))\n",
    "        for n in range(epochs):\n",
    "            for m in range(N):\n",
    "            # output layer\n",
    "                iCurrLayer = self.nLayers -1\n",
    "                currLayer = self.layers[iCurrLayer]\n",
    "                self.predict([X[m]])\n",
    "                delta = currLayer.dE_du(y[m])\n",
    "                du_dw = currLayer.in_vec\n",
    "                for j in range(currLayer.nOut):\n",
    "                    dw[iCurrLayer][j] = eta * delta[j] * du_dw\n",
    "                    db[iCurrLayer][j] = eta * delta[j]\n",
    "                nextDelta = delta\n",
    "                nextLayer = currLayer\n",
    "\n",
    "                # hidden layers\n",
    "                for iCurrLayer in range(self.nLayers-2, -1, -1):\n",
    "                    currLayer = self.layers[iCurrLayer]\n",
    "                    dE_do = []\n",
    "                    for n0 in range(currLayer.nOut):\n",
    "                        sDeltaW = nextDelta * nextLayer.w[:,n0]\n",
    "                        dE_do.append(sDeltaW.sum())\n",
    "                    delta = dE_do * currLayer.do_du()\n",
    "                    for j in range(currLayer.nOut):\n",
    "                        dw[iCurrLayer][j] = eta * delta[j]*du_dw\n",
    "                        db[iCurrLayer][j] = eta * delta[j]\n",
    "                    nextDelat = delta\n",
    "                    nextLayer = currLayer\n",
    "                for i in range(self.nLayers):\n",
    "                    self.layers[i].gd(dw[i], db[i], momentum)\n",
    "            # 학습 과정 출력\n",
    "            if n < 10 or (n+1) % 100 == 0 :\n",
    "                y_hat = self.predict(X)\n",
    "                print(f.format(n+1, self.loss(y,y_hat)))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
